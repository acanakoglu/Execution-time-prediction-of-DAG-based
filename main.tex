%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

%\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out
                                                          % if you need a4paper
\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                          % paper

\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{xcolor}
\usepackage{cite}

\usepackage{graphicx}

\usepackage{amssymb}

%added
\usepackage{booktabs}% http://ctan.org/pkg/booktabs
\newcommand{\tabitem}{~~\llap{\textendash}~~}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode}
\usepackage{multirow}
\usepackage{subfig}
\usepackage{makecell}

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\title{\LARGE \bf
Performance Prediction for Scientific Workflows on Spark. 
}
%Performance prediction for DAG-based Spark Applications mixing Machine Learning and Analytical Modeling. 

%\author{ \parbox{3 in}{\centering Huibert Kwakernaak*
%         \thanks{*Use the $\backslash$thanks command to put information here}\\
%         Faculty of Electrical Engineering, Mathematics and Computer Science\\
%         University of Twente\\
%         7500 AE Enschede, The Netherlands\\
%         {\tt\small h.kwakernaak@autsubmit.com}}
%         \hspace*{ 0.5 in}
%         \parbox{3 in}{ \centering Pradeep Misra**
%         \thanks{**The footnote marks may be inserted manually}\\
%        Department of Electrical Engineering \\
%         Wright State University\\
%         Dayton, OH 45435, USA\\
%         {\tt\small pmisra@cs.wright.edu}}
%}

\author{Andrea Gulino, Arif Canakoglu, Stefano Ceri and Danilo Ardagna}




\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

Spark is an in-memory cloud computing platform for implementing distributed applications of various types, such as scientific workflows.
Predicting the execution time of scientific workflows is an important but challenging problem that has been tackled in the past few years by several studies. However, those studies either focus on the single task, validating their models on simple applications (e.g. static applications, known ML algorithms, Spark-SQL-based applications), or just focus on the makespan estimation and scheduling problem, without showing the problems that arise when combining the two things (\textbf{e.g. XXX when intermediate results will be provided XXX }).
In this paper, we tackle the problem of modeling and predicting the performance of DAG-based applications implemented on Spark, in which Directly Acyclic Graphs (DAGs) describe how data flows through several tasks, each transforming the input data through a set of Spark operations.
We model the response time by combining prediction models built for each task in the DAG, mixing ML and analytical approaches in a complementary way. 
Prediction models take into account different features, such as the input data profiles, execution environment (CPUs and memory) and task-specific parameters.
We apply our methodology to ScQL, a complex system mapping SQL-like queries to DAG-based Spark Applications and we measure the accuracy of our predictions through an extensive experimentation on both artificial and real datasets. 

\end{abstract}

\begin{keywords}
Performance prediciton, workflow applicatons, Spark, Machine Learning
\end{keywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}
In the past decade we have witnessed an increasing spread of big data applications in several domains, such as business analytics \cite{sun2018big}, social media analysis \cite{ghani2019social}, healthcare \cite{kulkarni2020big} and natural language processing \cite{Hirschberg261} \textbf{XXX A bit old ref, I can provide more recent XXX}.
Big data applications are characterized by data intensive and computational intensive tasks which are typically implemented on top of highly parallel algorithms and distributed computing frameworks. Among such frameworks, Apache Spark has emerged as the de facto platform for analytical processing, with broad adoption in both industry and academia, due to its simplicity, fault tolerance and scalability \cite{Hirschberg261}.\\
At the same time, users and enterprises have started moving their big data applications from  traditional local server architectures to cloud computing platforms (e.g. Amazon EC2, Google Cloud, Microsoft Azure\footnote{https://aws.amazon.com, https://cloud.google.com and https://azure. microsoft .com }), which provide configurable environments suiting the needs of big data applications.
These systems usually offer services at the Platform level  where big data  frameworks are already installed and allow their users to choose among several configurations, e.g. specifying the number of instances in a cluster and their characteristics (CPUs, memory, \ldots).
Each choice might drastically affect  the application execution time and the monetary cost of using the cloud computing service. 
Resources allocated to each application should be proportional to its workloads, not only to avoid resources wasting but also considering that allocating more resources than necessary might negatively impact on the execution time \cite{li2015sparkbench}. \textbf{XXX this is not usual, provide a reference to support this statement XXX: Overcommitting CPU resources leads to CPU contention and adversely impact the execution time, cited paper. }
Predicting the performance of an application is therefore useful for a proper allocation of the available resources, aimed at reducing resource wasting and extra costs.

The problem of performance prediction for cloud big data applications has been tackled in several studies. Some  studies rely on traditional techniques, such as analytical models \cite{nelson1988approximate, mak, ardagna1, liang2000performance} and simulation \cite{bertoli2009jmt}.  \textbf{XXX These refs are old and too many, I'll provide something better and more recent here  XXX} More recently, machine learning (ML) has been used to predict the performance of large systems \cite{ernest, MUSTAFA20183767, pan2017hemingway, alipourfard2017cherrypick, ARDAGNA2019, lattuada2019gray} \textbf{XXX too may here too, we are out of page limit, select refs carefully XXX}.
The idea is to collect training data offline and use ML models, such as regression, to predict the runtime performance of future executions.
Those studies mainly differ for the chosen set of features (\textit{black-box} vs \textit{gray-box} approaches \textbf{XXX cite Maros XXX: done after this sentence}), i.e. capturing more or less details of the system,  and for the applied ML technique (simple regression vs more complex ML models). In \cite{maros2019machine} authors show how black-box models (specifically Ernest) fail when applications exhibit irregular patterns and/or when extrapolating on bigger data set sizes.
Despite achieving good results in terms of accuracy, all those studies are performed on simple monolithic applications (e,g. static programs, known ML algorithms), which complexity is far from modern data analytics pipelines.
Indeed, nowadays, scientific jobs are rather represented as workflow applications that consist of many complex tasks, with logical or data dependencies, that can be implemented on top of several parallel frameworks. \cite{atkinson2017scientific, warr2012scientific, lordan2014servicess} \textbf{XXX these are old too and I cannot be of help here but I suggest to look at COMPSs papers by Barcelona Computing Center and XXX: we added comps and another paper about scientific workflows}.
%https://books.google.it/books?id=xWoqBAAAQBAJ&lpg=PR3&hl=it&pg=PR3#v=onepage&q&f=false
\color{violet}

%Workflows can be implemented on top of platforms such as Apache Airflow (and  KNIME?), which
%Several workflow management platforms are available (e.g. Apache Airflow), and help users schedule and monitor the workflow, or implemented from scratch.
%Each task of a workflow can be then executed as an independent application (each task runs a separate process ... ) or the entire workflow can be mapped to a single application, e.g. being mapped to a single Spark Application. 

%In the last case, the dependencies of each task (incoming edges) represent its input RDDs. This avoids materializing the intermediate results, resulting in more efficient executions.
\color{black}




A workflow application is  represented by a Directed Acyclic Graph (DAG), i.e. a directed graph with no cycles, in which vertices and their connecting edges are used to represent, respectively,  tasks and their dependencies. A vertex (task) cannot be executed before all its parent tasks finish their execution and transfer their output data to their child tasks.
In a DAG we can distinguish among \textit{entry tasks}, corresponding to reading the input data of the workflow, and some \textit{exit tasks}, corresponding to the storage/visualization of its results.
Note that, differently from monolithic applications,  the input to each intermediate task of the DAG cannot be known a-priori.

We argue that a trivial performance estimation model considering workflows as single monolithic applications, with known inputs, would not be able to properly capture their internal complexity, resulting in poor accuracy.
Moreover, workflows can be dynamically generated as a combination of a set of pre-defined tasks. In that case, a ML model should be trained for each feasible combination of pre-defined tasks, which, in practice, would be unfeasible and not scalable.

In this work we propose a general model for estimating the execution time of workflow applications based on Spark, using ML \textbf{XXX I would define your approach  \textit{hybrid} as based on ML and analytical models, and the work would look more original than the number of ML based papers that are popping up today XXX}. Compared to previous works:
\begin{itemize}
    \item  We deal with complex and dynamic configurable workflows, rather than simple and monolithic applications, which are the typical benchmarks of previous works.
    \item We address both the problem of estimating the individual task execution time and combining sub-task estimation to predict the overall workflow execution time (make-span), while other works in the field typically address only one of the two problems.
    \item We tackle the problem of estimating the profile of intermediate data, which is necessary to estimate the execution time of intermediate tasks, and consequently the overall execution time estimation.
\end{itemize}

The general idea is to use ML to build the performance model of each task that might compose the workflow and then combining those estimates to predict the workflow make-span. Intermediate data profiles are estimated either analytically or building ML models \textbf{XXX say something more on the analytical model role XXX}.

This paper is organized as follows: in section \ref{section:related_work}, we present the related work; in section \ref{section:dag_applications} we formally describe DAG-based workflow applications and the steps required for performance estimation; in \ref{section:task_modeling} we describe how to predict the performance of each task and discuss intermediate data profile estimation; in \ref{section:overall} we explain how the workflow make-span is computed and in \ref{section:evaluation} we present our experimental evaluation.

\textbf{XXX General comment:  Spark is used for big data analyses and ML today/data science.  When you mention scientific workflows the MASCOTS community can think, e.g., to molecules simulations and the like.  Here I would make more explicit the class of applications you consider, which are not classic scientific applications as above.  There is a mismatch otherwise also in the way you model the DAG which is data-oriented (e.g., to simulate molecules you don't need probably to characterize the data model (relational/semistructured, etc., Sect 3 XXX} 



\section{Related Work}
\label{section:related_work}
To the best of our knowledge, this is the first comprehensive work on performance estimation for scientific workflows based on ML \textbf{XXX hybrid XXX}, with specific focus on Spark. Indeed, previous works have always been focused on either performance prediction for cloud-based applications (e.g. Spark Applications), or on the estimation of the workflow makespan. Moreover, there is no work explicitly addressing the problem of intermediate result estimation, that arises when combining the two aforementioned problems. 
Therefore, we divide the related work in two: performance prediction for Spark applications; makespan estimation for DAG-based applications. 

Performance prediction for cloud based applications has been tackled in several ways. 
Some studies apply traditional techniques, such as analytical models \cite{nelson1988approximate, mak, ardagna1, liang2000performance} and simulation \cite{bertoli2009jmt}. These techniques usually require detailed knowledge of the system, which is available only at runtime, and make  simplifying assumptions that  produce models unable to capture the complexity of cloud-based big data applications, losing in accuracy. More recently, there have been studies exploiting machine learning (ML) models for performance prediction of large systems \cite{ernest, MUSTAFA20183767, pan2017hemingway, alipourfard2017cherrypick, ARDAGNA2019, lattuada2019gray}.
Most of these works use a \textit{black-box} approach, in which historical data is used to predict future execution time, without knowledge of the system internals. Some works, instead, try to apply \textit{gray-box} approaches, taking into account some system detail \cite{ARDAGNA2019, lattuada2019gray, shon2008scientific}. 
\textit{Ernest} \cite{ernest}, using simple features (functions of the input size and of the number of cores) and non-negative least squares (NNLS) regression, is considered the state-of-the art in using ML. In \cite{ARDAGNA2019}, several ML models and several approaches (\textit{black-box} vs \textit{gray-box}) are compared, showing better accuracy w.r.t \textit{Ernest}.

The workflow scheduling problem is a well-known NP-complete problem \cite{hartmanis1982computers}. However, even if there is a quite a lot of work related  to optimal scheduling, only a limited amount of work deals with performance estimation, which, however,  cannot be easily decoupled / generalized w.r.t. to the specific scheduling algorithm used by the underlying WMS (Workflow Management System).
Most of the available estimation models deal with simple workflows having unit tasks or no dataflow latencies on homogeneus systems. Some works focus on prividing lower and upper bounds for the makespan \cite{al1990lower, jain1994lower, hu1994improved}. A level based approach partitions the DAG in levels and computes the makespan of each level, by combining / dividing the execution time of each task in a level. The makespan of all levels are summed up to produce an estimate of the overall workflow makespan.
Todo: complete the references \textbf{XXX and I would suggest more recent, also the last 3 above are rather old XXX}.


\section{DAG-based Workflow Applications}
\label{section:dag_applications}


A workflow application can be represented as a Directly Acyclyc Graph (DAG), i.e. a directed graph with no cycles. Each vertex in a DAG represents a task, while edges represent the data and control dependencies between tasks.  A task is executed only when all its input data have been computed.
%$G=(V,E,L)$
Formally, we describe the DAG representing a workflow application as a tuple $G=(V,E)$, where $V$ is the set of vertices (tasks), $E$ the set of directed edges (dependencies) s.t. $E\subseteq V^2$.
%, and $L$ a labelling $L: E \to \mathbb{N}$, assigning each edge $e=(v,w)\in E$ a label, uniquely identifying the output data of task $v$.
The aciclicity is given by assuming that the transitive closure $E^+$ of the relation $E$ is irreflexive; i.e. $(v, v) \not \in E ^+ ,\ \forall v \in V$. An edge  $e=(v,w)\in E$ is called \textit{outgoing edge} of vertex $v$ and \textit{incoming edge} of vertex $w$.

Although we do not constrain the number of incoming and outgoing edges of a task,
we assume that the output data generated by each task is unique.
%, i.e.: 
%$$\nexists \ e=(v,w), e'=(v,w') \in E \ | \ w\not=w' \wedge L(e)\not=L(e') $$
%meaning that the output data of a task is unique but can be dispatched to multiple child tasks.

In a given DAG, a vertex without incoming edges is called \textit{entry task}, while a vertex without outgoing edges is called \textit{exit task}. Usually, entry tasks represent the retrieval of the workflow input data, while exit tasks represent the storage / visualization of the workflow results.

Moreover we define the set of \textit{predecessors} of a task $v \in V$ as $pred(v) =\{ w | (w,v) \in E \} $ and the set of its \textit{successors} as $succ(v) =\{ w | (v,w) \in E \}$.
\textbf{XXX In the introduction you discussed the possibility to have mutiple exit task: This open the problem to predict performance of a path in the DAG or the average execution time of the full DAGs according to the probability distributiosn of its execution path.  The simple approach is remove in the intro mutiple exit tasks and here state explicitely that we focus on single exit task, i.e. we have a single execution path in the DAG XXX} 
\begin{figure}
  \centering
  \includegraphics[width=0.48\textwidth]{sources/task.png}
  \caption{Characterization of a generic task of the DAG ($v_3$). 
  %According to our definition $P_{3\rightarrow4}=P_{3\rightarrow5}$
  }
\label{fig:operator}
\end{figure}


Our definition of workflow DAG is enriched with the following metadata:
\begin{itemize}
    
    %\item \textit{Data Profile}:  $P_{i\rightarrow j} = \{p^{i\rightarrow j}_1, p^{i\rightarrow j}_2, \ldots,p^{i\rightarrow j}_{|P_{i\rightarrow j}|}\}$, associated to each edge $(i,j)\in E$, represents the profile, i.e. a set of features providing a quantitative description, of the output data generated by task $v_i$ and delivered to task $j$. We also define the set of input profiles  of a task $\Pi^{in}_i = \{ P_{j\rightarrow i} | (j,i) \in E\}$ and its dual  $\Pi^{out}_i = \{ P_{i\rightarrow j} | (i,k) \in E\}$, that will contain only a single profile ($|\Pi^{out}_i| \leq 1$), given that the output data of each task is assumed to be unique. The set of features in $P_{i\rightarrow j}$ follows the same schema for any $i$ and $j$.
    \item \textit{Data Profile}:  $P_{i} = \{p^{i}_1, p^{i}_2, \ldots,p^{i}_{|P_{i}|}\}$, associated to each task $v_i\in V$, represents the profile, i.e. a set of features providing a quantitative description, of the output data generated by task $v_i$ and delivered to all its successors $succ(v_i)$. We also define the set of input profiles  of a task $\Pi_i^{in} = \{ P_{j} | v_j \in pred(v_i)\}$. For any $v_i \in V$, features in $P_{i}$ have the same schema. \textbf{XXX Not clear here what you mean, you consider the same set of features? XXX}
    
    \item \textit{Task Parameters}: $R_{i} = \{r^i_1, r^i_2, \cdots, r^i_{|R_{i}|} \} $, associated to each task $i$, denotes the set of specific parameters for task $v_i$. Examples can be selectivity, filter option and so on ... not binning (see next point). The set of features in $R_i$ follows the same schema for any $i$.
    \item  \textit{Environment Parameters}: ($E_{i} = \{e^i_1, e^i_2, \cdots, e^i_{|E_{i}|} \} $), associated to each task $v_i$, denotes the set of features describing the execution environment used to run task  $v_i$ (e.g. number of CPUs, memory, partitions). In this set we include all those parameters that do not affect the result of the task, but just its performance (e.g. binning). The set of features in $E_i$ follows the same schema for any $i$.
    \item \textit{Execution Time}: ($t_i$), which applies to each task of the workflow. It corresponds to the time required for processing all the input data of task $v_i$  and to produce its (unique) output $L(v_i)$. As later discussed, this time depends on the input data, task and environment parameters. 
    \item \textit{Transfer Time} ($\tau_{i\rightarrow j}$), associated to each edge $(i,j)\in E$, represents the time required for moving the output of a task to its successors. An example might be network transfer  data time.
\end{itemize}


\color{red}
We should discuss what is a task in practice in a workflow based on Spark: e.g. a task encapsulates one or more Spark Jobs and the entire workflow is a single spark application, or each task could be a single Spark Application.
\color{black}

% andrea: should we say that there are vertices with no edge at all? 
\color{blue}
The modeling of the workflow as described above is independent of the input data types, which can be relational records, semi-structured documents or plain text items.
\color{black}

\subsection{Performance Estimation}
\label{subsec:pipeline}
Figure \ref{fig:module} shows the three phases characterizing the proposed performance estimation approach. Starting from:
\begin{itemize}
    \item a Workflow \textbf{DAG}: as previously described, in which, for each task $v_i$, the sets $R_i$ (task parameters) and $E_i$ (environment parameters) are already set.
    \item all \textbf{input profiles}: data profiles of the workflow input datasets, i.e. $P_i$ for each task $v_i$ that is an entry task. 
\end{itemize}
the response-time estimation is performed executing, in this order, the following steps:
\begin{enumerate}
    \item \textbf{Intermediate profile estimation}: for each task $v_i$, which is not an entry task, the output data profile is estimated. This is discussed in \ref{section:intermediate_input}.
     %\item (maybe here there is a phase in which we set, for each task $v_i$, the actual number of resources that can be used (i.e. we set $E_i$), depending on the number of other tasks which can be run in parallel with $v_i$. 
    \item \textbf{Tasks' execution time estimation}: for each task of the DAG, its execution time is predicted. This is discussed in section \ref{section:task_modeling}.
     \item \textbf{Response time estimation}: the overall response time is computed by combining the individual task execution time estimations \textbf{XXX how do you estimate transfer time? XXX}. This is discussed in section \ref{section:overall}.
\end{enumerate}


\begin{figure}
  \centering
  \includegraphics[width=0.38\textwidth]{sources/perf-estimation-module.png}
  \caption{Steps performed by the performance estimation module to produce an estimation of the workflow response time, given the workflow DAG and the input profiles.}.
\label{fig:module}
\end{figure}

\section{Task execution time estimation}
\label{section:task_modeling}
In this section we describe how the execution time of a task can be predicted by machine learning. However, the approach described in this section is used also to the output-data profile estimation discussed in \ref{section:intermediate_input}. 
The idea is to build, for each different type of task that can be present in a workflow, a machine learning model that, with \textit{adequate} accuracy, is able to predict the task execution time \textbf{XXX (output data size) XXX}. The ML model is trained on a set of features characterizing the task, such as a quantitative description of its input data and parameters, and the execution environment. \\
In this section, we give particular importance to the choice  of features, proposing a categorization that depends on the knowledge of the application, its data model and the execution environment, and on the complexity of the defined features.

We assume that the input data profile of the task is available. In section \ref{section:intermediate_input} we discuss how those profiles can be estimated.        


\subsection{Feature Categorization}
\label{subsec:feature-set}
The accuracy of a machine learning model depends on the set of features which has been chosen to train that model. In general, the more your feature-set resembles the set of features composing the (unknown) underlying model, the higher will be the model accuracy. 
However, identifying and extracting a complete set of features is hard to achieve. First of all, your knowledge on the process you wish to describe might be limited; secondly, extracting some features might be non-trivial and time-consuming, hence not convenient for the purposes. 

When dealing with performance estimation, feasible solutions are usually based on simple feature sets that provide an high level description of the data and of the execution environment. 
Most of the previous works have considered the so-called \textit{black-box} features, e.g. data size and number of cores, which can describe at an high level any type of application.
Although it has been shown that even with simple features it is possible to achieve good accuracy, a simple choice of features might be limiting when trying to describe the performance of more complex applications, which are the target of this work.

In this paper we consider different feature-sets, showing how different choices can impact the prediction accuracy of the produced models. Without constraining the choice of features, we propose a categorization of the possible feature-sets.

First of all we make a "\textit{knowledge-based}" distinction between features. We call them as black-box and gray-box as defined below:
\begin{itemize}
    \item \textbf{Black-box features} do not require detailed knowledge on the application, data model and execution environment;
    \item \textbf{Gray-box features} 
    extend black-box features with application-specific, data-model-specific and environment-specific features. For convenience, we assume that the gray-box feature-set always includes black-box features.
\end{itemize}
Orthogonality to the previous distinction, we discriminate among:
\begin{itemize}
    \item \textbf{Basic features}, organized into:
    \begin{itemize}
        \item \textbf{Input Data} descriptive features: Assuming that our objective is to build a prediction model for a task $v_i$, this set is made of those features quantitatively describing the input data of $v_i$ (e.g. data size, number of files, number or entries $\cdots$). This corresponds to the set of input data profiles $\Pi^{in}_i$ mentioned in section \ref{section:dag_applications}.
        \item \textbf{Task Parameters}: a set of features representing the arguments of the task. For a task $v_i$, it is equivalent to the set $R_i$ mentioned in  section \ref{section:dag_applications}.
        \item \textbf{Execution Environment} descriptive features: a set of features representing the environment in which the task is executed (e.g. number of CPUs and amount of memory). For a task $v_i$, it is equivalent to the set $E_i$ in  section \ref{section:dag_applications}.
    \end{itemize}
    \item \textbf{Composite features}. Even though several machine learning methods are able to capture non-linear dependencies on the features, it is sometimes "helpful" to include non-linear combinations of some basic features in the final feature set. For, example \textbf{XXX Ernest cite XXX} introduced the logarithm of the number of cores which encodes the cost of reducing operations in parallel operations \textbf{XXX Add also the discussion on data size/ number of cores since otherwise it is not clear that you perform combinations below XXX}. We define composite features as linear or non-linear combinations of basic features.
\end{itemize}

\input{table-feature-sets}

Given the aforementioned distinctions, we organize the possible feature sets in four categories, schematized in Table \ref{tab:feature-sets}: 
\begin{itemize}
    \item Basic black-box features (\textbf{BB-Basic});
    \item Basic and composite black-box features (\textbf{BB-Full});
     \item Basic gray-box features (\textbf{GB-Basic});
    \item Basic and composite gray-box features (\textbf{GB-Full}).
\end{itemize}
In Table \ref{tab:feature-sets-instance}, we provide an instance for each of these four categories.

For each category, we added all the candidate features, possibly non-relevant features as well. A proper feature selection algorithm easily removes useless ones and reports them with their importance.   
%What we denote as feature-set should be considered an initial set of features which may also include non-relevant features. A proper feature-selection algorithm should be used to remove any non-relevant feature from the set.

% ARIF: we can change the reference and move this sentence to section intermediate_input
In \ref{section:intermediate_input}, we use the same feature-set distinction for predicting the output data profile of a task, excluding execution environment features.

\subsection{Model Techniques}

\textbf{XXX This section is generic, focus and state what you used XXX}

In the previous works, Venkararaman et al.(Ernest) et al. \cite{ernest} and Maros et al.\cite{ARDAGNA2019} have applied ML to estimate the execution of simple Spark applications. Ernest
is based on linear regression, with coefficients estimation based on  non-negative least squares (NNLS). 
%This is a variant of the least squares algorithm with the added restriction that coefficients must be non-negative. This restriction ensures that each term provides a non-negative contribution to the overall execution time, preventing estimation of negative execution times.
In \cite{ARDAGNA2019}, four classic ML techniques have been compared, including L1-regularized linear regression (LASSO), neural networks, decision trees, and random forest. \\
Each model technique has its own characteristics \cite{bishop2006pattern}:
\begin{itemize}
    \item Linear regression (LR) produces models which can be easily interpreted, but hardly captures complex interactions between features.
    \item Decision Trees (DT) and Random Forest (RF), have the ability to capture non-linear relationships, still allowing a good interpretability of the model.
    \item Neural Networks (NN) can capture non-trivial interactions among the input features albeit less interpretable.
\end{itemize}

In the experimental section we analyze the performance of different model techniques on different feature-sets, showing the advantages and drawbacks of using one model w.r.t. another and testing the model robustness against the variation of some relevant features  (e.g. data size and number of CPUs).


\subsection{Data Collection}
Given a target task type, for which we want to predict the execution time, the training set provided to the chosen machine learning technique is made of several executions of that task.
Each execution should be described by:
\begin{itemize}
    \item All the features contained in the selected feature-set
    \item The execution time
    \item Features describing the output data, coherently with the features chosen to describe the input data. These features are required if the output estimation is performed by machine learning (see section \ref{section:intermediate_input}).
\end{itemize}
Although some machine learning methods are quite robust to noise, it is advised to use a clean execution environment entirely dedicated to training data collection. To further eliminate noise, the task should be the only task in the DAG (unless some entry task or exit task is necessary).

As we prove in the experimental section, depending on the feature-set and on the model technique, it might not be necessary to run on big inputs and extremely powerful environments. Even running many short executions on medium-size resources could still produce prediction models with stable prediction error.


\subsection{Training and Test Sets}

ARDAGNA2019\\
\color{blue}
To learn and evaluate the ML models, data coming from the experiments are split into training and test sets. The former is used to learn the model, whereas the latter is used to evaluate its accuracy. Since hyper-parameters tuning is performed for each ML technique (see Section IV-D), a sub- set of the training data is used for cross-validation to reduce over-fitting. For each workload, we evaluate the accuracy of the prediction models in terms of core interpolation and data size extrapolation capabilities, acknowledging the fact that the data available for learning the models (training set) might have been obtained via experiments on setups different from the one for which the prediction is needed (test set). Figure 1 and Table II summarize the scenarios considered.
In the core interpolation scenarios, we consider runs with the same dataset size (reported in Table II) and verify the capabilities of the trained models of interpolating the number of cores. Figure 1 shows the various scenarios (y-axis) of core interpolation built for each workload based on different splits of the data into training and test sets: in each row (case number), blue boxes represent configurations for which the data were used as part of the training set (and cross- validation) and red crosses indicate configurations used as part of the test set10. We designed scenarios such that larger case numbers are associated with harder predictions, as their training data include samples from a smaller range of experiments w.r.t. the number of cores. In the data size extrapolation scenarios, we put the runs with the largest dataset size (spanning across all available cores configurations) in the test set while the runs with the other dataset sizes in the training data, as shown in the two rightmost columns of Table II. Moreover, training sets are further reduced by removing runs according to the same schema presented for core interpolation. By doing so, in these experiments we evaluate at the same time the core interpolation and the data size extrapolation capabilities. In other words, these experiments differ from the core interpolation scenarios because: (i) the dataset sizes in training and test sets are no longer the same, (ii) in the test set we also include observations where the number of cores is the same as in some observations of the training set (but again with different dataset sizes).
\color{black}



\color{black}
\subsection{Predicting intermediate input features}
\input{procedure}
\label{section:intermediate_input}
When we described how the execution time of a task should be predicted, we assumed that all its input profiles were available. However, input data of a task are known only when all predecessors tasks have completed their executions.\\
Since performance prediction is done before actually running the workflow, it becomes necessary to estimate all the tasks' input profiles  offline.\\
In the following, we assume that the input data of entry tasks are known and already profiled, thus not impacting neither on the run-time nor on the performance-prediction time. This assumption is in general acceptable, since:
\begin{itemize}
    \item the application users tipically download a limited number of datatasets which are re-used for multiple workflows, or, at least, several runs of the same workflow (e.g. when fixing / improving the workflow). The data could be profiled once imported, and the profile would be available for multiple executions.
    %\item The profiling task is usually simpler and less time consuming w.r.t the workflow that uses it; i.e. the profiling time is neglectable w.r.t. to the workflow execution time.
    \item  even this initial profiling could be performed approximately in a short time. E.g., Haas et al.\cite{haas1995sampling} base their estimation on data samples, empirically comparing various estimators from literature. 
    %This way of profiling data is less time consuming and could be also a good runtime solution in case offline profiling resulted unfeasible. 
    However, a low accuracy of this estimation would negatively affect the quality of performance prediction.
\end{itemize}

 The goal is to estimate, for every task $v_i$ in the workflow its output profile $P_i$, i.e. building a model for each feature in $P_i$, given:
\begin{itemize}
    \item the input profiles of task $v_i$, i.e. $\Pi^{in}_i$
    \item the set of task parameters $R_i$
\end{itemize}{}

\noindent As previously discussed, depending on the knowledge on the application / data model, your feature set might be more or less detailed.
The same categorization proposed in  \ref{subsec:feature-set} can be applied, with the reasonable assumption of excluding execution environment features and any related composite feature.
%For instance, if a task just converts each row of a table into another format, and the profile just describes the number of rows in the datasets, the estimation would be very trivial. 
%Consider, instead, a task implementing a JOIN  between two datasets, with a condition on the values. In that case, the profile might also include statistics on the values  (e.g. cardinality, distribution), and have the predicate encoded as task parameters. An accurate model for output estimation, in that case, should take into account the \textit{selectivity} of the predicate w.r.t to the data to which it is applied. 

Each feature in $P_i$ can be estimated:
\begin{itemize}
    \item \textbf{Analytically}: there is a known combination of features of the chosen feature-set which allows to exactly compute the output feature.
    \item \textbf{Heuristically}: the exact formula for computing the output feature is unknown or there is not enough information to compute it; in this case,  heuristics on the available feature-set can be defined.
    \item by \textbf{Machine Learning}: applying ML to estimate the output feature, similarly to what was has been said  for task execution time prediction.
\end{itemize}

\noindent For convenience, we define the procedure:
\begin{equation}
\label{eq:predict}
predict(i, p, \Pi_i^{in}, R_i)
\end{equation}

\noindent which, given a task $v_i$, its input profiles $\Pi_i^{in}$ and task parameters $R_i$, returns the estimated output profile $P_i$.
Finally, as mentioned in \ref{subsec:pipeline}, before predicting tasks' execution time, all the input profiles are  estimated by calling the recursive procedure \textproc{profileEstimation}(e) for each $e \in V$ that is an exit task of the workflow.

\textbf{XXX Here it is not clear what you do in concrete. The problem its clear but not its solution XXX}

\section{Workflow Makespan Estimation}
\label{section:overall}

The makespan of a workflow is the time required to execute all the tasks in a workflow. In general, the makespan is lower bounded by a minimum time given by the longest path from any entry task to any exit task (critical path), and upper bounded by the time spent to execute all the tasks sequentially (including transfer costs). \textbf{XXX If you have mulitple exit tasks is still critical path a lower bound for excetuion ? XXX}

The  makespan is tightly dependent on the topology of the workflow and on the scheduling algorithm used by the WMSM, that, given that the scheduling problem is NP-complete, applies heuristics to produce minimal cost schedules.

Although using ML techniques, might be a good way to estimate the makespan of a workflow on a generic workflow system, in this paper we focus on workflows built on top of Spark. Assuming that :
\begin{itemize}
\item every task is executed within the same spark context
\item every task is executed using all the available resources
\end{itemize}
we can reasonably approximate the makespan with the following formula:
$$ \sum_{i\in V} t_i + \sum_{(i,j) \in E} \tau_{i\rightarrow j}$$
i.e. summing the individual task execution times and transfer costs (which we assume to be zero).  \textbf{XXX so data transfer are zero? I would state at the beginning and I would not add complexity to the model.  Moreveor, do you assume that all tasks are sequential? XXX}


%Notes on Spark scheduling:

%In a Spark job, multiple independent stages (e.g., stage 0 and stage 1 to 14 in App 2 as shown in Fig. 1(b)) can be executed simultaneously. Spark standalone mode provides the following two schedulers to manage the execution of multiple stages.
%- FIFO: (first-in-first-out) assigns the resources based on the stage submission order. When all the tasks in a stage are served, FIFO starts to consider the resource assignment for the next stage.
%- FAIR: allows the system to assign different (fixed) weights to jobs and group them into pools based on their weights [3]. However, we find that neither of them can work well with complex data flows. First, the performance of FIFO heavily depends on the order of stage submission which is often decided by user’s application codes. A slight difference in the user codes with the same function can yield a significant difference in the performance1. In addition, while serializing the execution of all the stages, FIFO does not use the knowl- edge of the DAG structure to determine the execution order of the stages. This can certainly cause delays at the stages that need the input from other stages. Second, while FAIR allows parallel execution of multiple stages, it does not carefully consider the DAG structure, either. Moreover, the resources are equally distributed among the stages without considering their individual workload. As a result, FAIR may mitigate the performance issue in FIFO under some cases, but under other cases, as we show in Section II-D, FAIR could perform even worse than FIFO.


%Layered model for estimating worflow makespan:


%\color{blue}
%A model for estimating the makespan of a scientific workflow takes into account the workflow structure and divides tasks into levels based on the dependencies between them, so so that tasks assigned to each level are independent to each other. Then, for each level, its execution time (which is equal to the time required for the execution of the tasks in that level) can be calculated considering the overall runtime of the tasks of that level (that is the sum of the individual tasks runtime). The assignment in levels can be done using either a top-down or a  bottom-up approach that assigns a level to each task by taking into account the level of its predecessors or successors respectively. Once the assignment has taken place, a level-based estimation model takes into account level characteristics to provide an overall performance estimate for the workflow.
%\cite{pietri2014performance}

%control flow / data flow

%During execution, we assume that the system schedules as many resources as required to run a single vertex. As such, the vertices are executed sequentially. This is orthog- onal to the fact that within a vertex, execution is performed in a both pipelined and partitioned parallelism manner.
%\cite{gounaris2017dynamic}


%A New Makespan Estimation Model for Scientific Workflows on Heterogeneous Processing Systems

%https://ieeexplore.ieee.org/abstract/document/8411037


%The execution machine is exclusively dedicated to the data flow execution. I.e., we assume that an execution machine executes only one data flow and the execution of the next flow can be started only after the completion of the previous flow. So, the available machine executes tasks and stores data for a single data flow at a time.
%gao2017autopath
%\cite{gao2017autopath}



\section{Experimental Evaluation}
\label{section:evaluation}
Todo: introduce this section
XXX this introduction is very critical to contextualize your work.  I would organize as follow:  XXX


\begin{itemize}
\item Application 
\item Application profiling and training set collection:  environments, number of runs, etc.
\item Setup of hyper parameters and MAPE
\item Description of the analysis:  single task estimate and DAG, which DAG did you consider?  Introduce their representation, introduce extrapolation analysis on data size and number of available cores 
\item results in single tasks
\item results on data output prediction accuracy
\item results on DAG accuracy
\item Discussion on extrapolation analysis
\item. Whenever is possible comaprison with Ernest 
\end{itemize}
\subsection{Target Application}

\textbf{XXX Extend the description here XXX}

Query languages that can be mapped to a relation algebra can be represented as trees, i.e. DAGs / workflows.
For our experimentation, we chose a cloud-based system implementing a SQL-like language for interval data called ScQL \cite{ScQL}. ScQL queries are mapped to DAG-based Spark Applications.
%query is translated into a workflow in which each task implements a ScQL operator on Spark.

Although ScQL includes several operators, for this experiment, we focus on the two most complex and peculiar operators: ScQL-Map and ScQL-Join, along with ScQL-Select and ScQL-Materialize, which are mandatory entry and exit tasks (for reading inputs and storing results).   \\

\noindent\textbf{ScQL Data Model}\\
An interval-dataset $D$ consists of several files. Each entry in a file represents an interval, by means of its start and stop values.\\

\noindent\textbf{ScQL Map}\\
This operator (task) takes two input datasets, namely a \textit{reference} and an \textit{experiment} dataset and computes, for each interval in the reference dataset, aggregates on the overlapping experiments intervals. \\

\noindent\textbf{ScQL Join}\\
This operator (task) takes two input datasets, namely a \textit{reference} and an \textit{experiment} dataset. It outputs, for each couple of reference-experiment files, the couples of overlapping reference-experiment intervals.  A \texttt{distance} parameter can be provided to match also experiment intervals which are at a given distance from a reference interval.  \\

\noindent\textbf{Binning}\\
A data-model-specific partitioning algorithm, called interval-binning, is used to process in parallel intervals. The \textit{bin-size} parameter determines the amount of intervals processed at each partition. Some intervals are replicated as a consequence of binning.

%We don't consider metadata computation, that is performed in a short period before query execution and just filters the initial inputs. Therefore we deal only with an equivalent workflow DAG made of only operations which are applied to observations.


\subsection{Environment}
We run our experiments on Amazon AWS, using the EMR service with a variable number of \texttt{r5d.2xlarge} instances (8 vCores, 64 GiB RAM). Specifically we used clusters with 6,8,10 and 12 worker nodes, using the \texttt{emr-5.19.1} release with \texttt{Spark 2.3.2} and the \texttt{Amazon 2.8.5} Hadoop distribution.
%Moreover, we used single machine (SM) provided by the CINECA computing centre, with 72vCores, 213GB RAM, Spark 2.3.2, on Ubuntu 18.04.4 LTS.

\textbf{XXX Results here are all based on AWS?  Did you drop your internal node?  Below you mention single node:  stress this cover a continuum where you run you application on a large multi-core node and distributed cluster to validate your approach   XXX}

\subsection{Results}
%todo: talk about random data generation
We measure the prediction error reporting the Mean Absolute Prediction Error (MAPE), defined as:
$$ \frac{1}{n} \sum_{t=1}^n \bigg| \frac{A_t - F_t}{A_t}\bigg|$$
where $A_t$ is the actual value and $F_t$ is the forecast value.

We report the validation error measured with 5-Fold cross validation X\textbf{XX for hyper parameter tuning?  XXX.}
Tables \ref{tab:map-aws}, \ref{tab:map-sm},  \ref{tab:join-aws} and \ref{tab:join-sm} report the measured prediction error for different ScQL operators, i.e. ScQL-Map (tables \ref{tab:map-aws} and \ref{tab:map-sm}) and ScQL-Join (tables \ref{tab:join-aws} and \ref{tab:join-sm}) and different execution environments, i.e. AWS (tables \ref{tab:map-aws} and \ref{tab:join-aws}) and single machine (SM) (tables \ref{tab:join-aws} and \ref{tab:join-sm}), in which we set the Spark Master to \texttt{local[n]}, with $n$ equal to the number of available CPUs. \\
Within each table, we report the prediction error for:
\begin{itemize}
    \item different choices of the feature-set, i.e. black-box vs gray-box and basic vs full, as described in section \ref{subsec:feature-set};
    \item different machine learning methods, including Decision Tree (DT), Random Forest (RF) and Linear Regression (LR). We excluded methods for which the best MAPE was higher than 0.2 (20\%), like XGBoost and Neural Networks.
\end{itemize}

\input{tables-mape}
\input{table-features}

Tables \ref{tab:relevant-features-map} and  \ref{tab:relevant-features-join}, show, respectively, the relevant features for ScQL-Map and ScQL-Join provided by the ML technique giving the lowest MAPE (Random Forest), for a given feature-set choice (black-box vs gray-box, basic vs composite).
The feature called \texttt{binned-result-size} represents the (known-by-semantics) result size, accounting for the replication of some intervals due to binning. The definition of this composite feature includes gray-box features such as the \texttt{bin-size} and the \textit{average-interval-length}.
Similarly applies to the feature called \texttt{binned-total-size}, which definition includes also a task parameter \texttt{distance}.

\noindent Results show that:
\begin{itemize}
    \item Random Forest is able to give good predictions even with black-box basic features. Therefore,  even without having a detailed knowledge on the application and on the environment, and without "helping" the model by defining composite (i.e. non-linear) features, it is still possible to get an acceptable error. 
    \item Linear Regression performs well only when all the complex the features are in place (gray-box), including the user-provided non-linear features (composite). 
    \item Basic gray-box features do not significantly reduce the prediction error unless they are properly combined into composite features. 
    \item The higher complexity of the ScQL-Join operation w.r.t the ScQL-Map is reflected in the higher prediction error.
    \item Single machine executions, using local master option, are much less predictable than executions performed on a well configured AWS cluster using Yarn.
    \color{blue}
    %Todo: our single machine (cineca) showed strange behaviours, we will re-rerun on an EC2 machine. 
    \color{black}
\end{itemize}


%Table with MAPE, MAP-JOIN / LOCAL-AWS
\begin{figure*}
  \centering
 \hspace*{-1cm}\begin{tabular}{ccc}
\subfloat[]{
    \includegraphics[width = 0.33\textwidth]{sources/Data size distribution (ScQL-MAP).png}
} &
\subfloat[]{
\includegraphics[width = 0.33\textwidth]{sources/ScQL-MAP - LR.png}
} &
\subfloat[]{
    \includegraphics[width = 0.33\textwidth]{sources/ScQL-MAP - RF.png}
}  \\
\subfloat[]{
    \includegraphics[width = 0.33\textwidth]{sources/Data size distribution (ScQL-JOIN).png}
}  &
\subfloat[]{
    \includegraphics[width = 0.33\textwidth]{sources/ScQL-JOIN - LR.png}
} &
\subfloat[]{
    \includegraphics[width = 0.33\textwidth]{sources/ScQL-JOIN - RF.png}
} \\
\subfloat[]{
    \includegraphics[width = 0.33\textwidth]{sources/cores_distribution_map.png}
}  &
\subfloat[]{
    \includegraphics[width = 0.33\textwidth]{sources/ScQL-MAP - LR-cores.png}
} &
\subfloat[]{
    \includegraphics[width = 0.33\textwidth]{sources/ScQL-MAP - RF-cores.png}
} \\
\subfloat[]{
    \includegraphics[width = 0.33\textwidth]{sources/cores_distribution_join.png}
}  &
\subfloat[]{
    \includegraphics[width = 0.33\textwidth]{sources/ScQL-JOIN - LR-cores.png}
} &
\subfloat[]{
    \includegraphics[width = 0.33\textwidth]{sources/ScQL-JOIN - RF-cores.png}
} 
\end{tabular}
\caption{Model robustness w.r.t the scaling of the input data size and the number of cores.}
\label{fig:scalability}
\end{figure*}


\subsection{Hyper-parameters tuning}
\input{table-hyper}
A library (todo: cite the library, \color{blue}
@Danilo, how do we cite it?
\color{black}) built on top of scikit-learn 0.19.1 \footnote{https://scikit-learn.org} was used to test different values for the hyper-parameters characterizing each learning methods.
Several combination of values for the hyper-parameters were tested and the best combination, corresponding to the lowest MAPE obtained with 5-fold cross validation, was selected.
The most frequently used hyper-parameters are reported in table \ref{tab:hyper-parameters}. 
Some parameters, e.g. the alpha penalty and the maximum three depth for Random Forest and Decision Tree help preventing overfitting. Minimum Samples to Split/per Leaf represent, respectively, the minimum number of samples required to split a node and the minimum number of samples required to be a leaf. More details on the hyper-parameters are provided in the scikit-learn documentation.




\subsection{Robustness w.r.t. increasing data and cluster size}
A desirable way to build a prediction model for performance estimation would consist in learning with small input data and few computational resources and expect a low prediction error even when the input data is much bigger and the execution environment is more powerful. In this way, the training dataset could be built in a short-time and renting expensive resources would not be necessary.
If a ML model is able to guarantee a stable prediction error for "unseen" value ranges of a given feature, we can say that the model is robust against the scaling of that feature.
In this section we compare the robustness of models, built with different  ML techniques (Random Forest and Linear Regression) and trained on different feature-sets, w.r.t. the scaling of the number of cores and of the input data size. \textbf{XXX Motivate why only RF and LR, most accurate? XXX} \\
In the first two rows of Fig. \ref{fig:scalability}, we tested the robustness against the scaling of the input data size for ScQL-MAP (first row) and ScQL-Join (second row). The first plot in each row shows on the x-axis the input data size and on the y-axis the number of executions in our dataset performed with an input of that size.
We trained the models on small executions (with input size lower than $x_0$ = 20\% of the maximum size) and measured the validation MAPE on executions belonging to unseen input size ranges: 20-40\%, 20-60\%, 20-80\% and 20-100\% of the maximum input size.
The scaling of the MAPE is depicted in the plots belonging to the second and third columns of Fig. \ref{fig:scalability}. The first point in each plot (positioned at $x_0$) represents the validation MAPE computed on some executions randomly selected  from the 0-20\% range that were not used for training. \\
Results show that Random Forest guarantees a good scaling, while Linear Regression fails (overfits in 0-20\% range), unless gray-box full features are provided. 

Similarly, we tested the robustness against the scaling of the number of cores, which, for our experiments, corresponds to the scaling of the number of worker nodes in the AWS cluster. We trained our models on executions with 16 and 32 cores (i.e. 2 and 4 worker nodes), and measured the validation MAPE on executions using 48 cores, (48 \& 64) cores and (48 \& 64 \& 96) cores.\\
Again, results show that Random Forest guarantees a good scaling, while Linear Regression fails (overfits), unless composite features are used. In this case LR can is robust even using black-box features, given that non-linear features involving the number of cores are defined (in our feature-sets is \texttt{input-total-size/cores}).
\color{blue}
Eventually, we observed that Decision Tree models are as robust as the models produced by Random Forest.
\color{black}



\subsection{Intermediate profiles estimation}
In Table \ref{tab:output-estimation} we report the prediction error (MAPE) for the main features describing the result of a ScQL-Join operation.
While the output number of files can be analytically determined, the output total size and the output average interval length cannot be known a priori, since they depend on the number of intersections between input intervals. For those features we defined two heuristics which give a low prediction error. For all the three features we reported the prediction error obtained using ML models (specifically, Decision Tree). Although the output total size is a black box feature, the prediction error reported in the table was obtained using a model which took into account gray box features; using only black-box features we were not able to obtain a  MAPE lower than 0.22. We don't report ScQL-Map prediction errors since all the output features can be easily estimated, both analytically and by applying ML.


\input{table-output-estimation}


\subsection{Makespan Estimation}

In order to measure the make-span prediction error we automatically generated DAGs with different topologies including ScQL-Map and ScQL-Join, together with other mandatory entry/exit tasks which were not discussed in this paper. As discussed in section \ref{subsec:pipeline}, we first estimated the input profiles for all the tasks in the DAG and then predicted each task execution time using ML models that we previously built. The make-span in estimated as discussed in section.
\color{blue}
We are currently runnng on AWS. On 88 executions with DAG depth varying from 2 to 6 the measured MAPE is 0.15.

\ref{section:overall}.

%Table with hyper parameter tuning




\section{CONCLUSIONS}
%Robustness of some methods w.r.t. others
TODO.


\section*{ACKNOWLEDGMENT}
This work was supported by the AWS Cloud Credits for Research program.
\bibliography{b}
\bibliographystyle{acm} 


\end{document}


%TODO metti reference a paper binning e GMQL